{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nfrom fastai.imports import *\nfrom fastai.structured import *\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom statistics import stdev\nimport math",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f43e97f28d739c05ce33aaf44f5579f4af9f648b"
      },
      "cell_type": "code",
      "source": "def rmse(actual, predicted):\n    return np.sqrt(np.mean((actual - predicted)**2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1926a67bb7bf44caa9488c882db7c9f96d3399f"
      },
      "cell_type": "code",
      "source": "class RandomForest:\n    def __init__(self, sample_size, n_trees = 10 , min_samples_leaf = 5, max_depth = 4, max_features = None):\n        self.n_trees, self.sample_size , self.min_samples_leaf, self.max_depth, self.max_features = n_trees, sample_size, min_samples_leaf, max_depth, max_features\n        self.trees = [self.create_tree() for _ in range(self.n_trees)]\n        \n    def create_tree(self):\n        \n        return DecisionTree(min_samples_leaf = self.min_samples_leaf, max_depth = self.max_depth, max_features = self.max_features)\n    \n    def fit(self, X, y):    \n        for tree in self.trees:\n            random_idxs = np.random.permutation(X.shape[0])[:self.sample_size]\n            tree.var_split(X.iloc[random_idxs, :], y[random_idxs]) \n    \n    def predict(self, x):\n        return np.mean([t.predict(x) for t in self.trees], axis = 0)\n\n    #exploring feature importances\n    def plot_pdp(self, X, y, feature_names, n_clusters = 0): pass\n    \n    def find_feature_importances(self, X, y): pass\n    \n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab52b00e328fabf8a399b79c8dd0a1b5c0fea801"
      },
      "cell_type": "code",
      "source": "%load_ext cython",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24bac0534be8d9a49e1f032f65c25be499ae4b2f"
      },
      "cell_type": "code",
      "source": "%%cython\nimport math\nimport numpy as np\ndef stddev(cnt, values, sum1):\n    return math.sqrt(math.fabs((values/cnt) - (sum1/cnt)**2))\n\nclass DecisionTree:\n    \n    def __init__(self, int min_samples_leaf = 3, int max_depth = 4, int level = 0, max_features = None, float parent_value = float('inf')):\n        self.min_samples_leaf, self.max_depth = min_samples_leaf, max_depth\n        self.score = float('inf')\n        self.value = 0.0\n        self.leftTree , self.rightTree, self.split_val, self.split_feature  = None, None, None, None\n        self.level = level\n        self.max_features = max_features\n        self.parent_value  = parent_value\n       \n       \n        \n    @property \n    def is_leaf(self):\n        return self.score == float('inf')\n    \n   \n    def find_better_split(self, x, y, int ind):\n    \n        x1 = x.values[:, ind]\n        inds = np.argsort(x1, axis = 0)\n        sorted_y, sorted_x = y[inds], x1[inds]\n        rhs_count, rhs_sum, rhs_sum2  = x.shape[0], sorted_y.sum(), (sorted_y**2).sum() \n        lhs_count, lhs_sum, lhs_sum2 = 0, 0., 0.\n      #  print('for '+ str(x.columns[ind]))\n        for i in range(0, x.shape[0]-self.min_samples_leaf+1):\n            lhs_count, lhs_sum, lhs_sum2 = lhs_count+1, lhs_sum + sorted_y[i], lhs_sum2 + sorted_y[i]**2\n            rhs_count, rhs_sum, rhs_sum2 = rhs_count-1, rhs_sum - sorted_y[i], rhs_sum2 - sorted_y[i]**2\n            if i < self.min_samples_leaf - 1 or sorted_x[i] == sorted_x[i+1]:\n                continue\n            updated_score = ((lhs_count * stddev(lhs_count, lhs_sum2, lhs_sum)) + (rhs_count * stddev(rhs_count, rhs_sum2, rhs_sum)))/(x.shape[0])\n            #print('score: '+ str(updated_score))\n            if updated_score < self.score :\n                self.score = updated_score\n                self.split_feature = x.columns[ind]\n                self.split_val = i\n                self.value = (np.mean(y[:i])*i + np.mean(y[i:])*(x.shape[0] - i))/(x.shape[0])\n        self.score = self.score\n       \n        \n   \n    def var_split(self, x, y):\n        \n        if x.shape[0] > self.min_samples_leaf and self.level < self.max_depth - 1:\n            #find max_features to split on\n             \n            if self.max_features is not None:\n                if self.max_features in ['auto', 'sqrt']:\n                    self.max_features = int(math.sqrt(x.shape[1]))\n                else:\n                    if self.max_features == 'log2':\n                        self.max_features = int(np.log(float(x.shape[1]))/np.log(2))\n                    else:\n                        if isinstance(self.max_features, float):\n                            self.max_features = int(self.max_features * x.shape[1])\n                        else:\n                            self.max_features = x.shape[1]\n            else:\n                self.max_features = x.shape[1]\n            self.max_features = int(self.max_features)\n            feature_inds = np.random.permutation(x.shape[1])[:self.max_features] \n            #print('will split on features: '+str(feature_inds))\n            feature_inds = [index for index in feature_inds if x.columns[index] != None]\n            for ind in feature_inds:\n                self.find_better_split(x, y, ind) \n           # print('split on '+ str(self.split_feature)+' with score '+str(self.score) + \"at level : \"+str(self.level))\n            if self.parent_value == float('inf'):\n                self.parent_value  = self.value\n            x_lhs, x_rhs = x.iloc[:self.split_val,:], x.iloc[self.split_val:,:]\n            self.leftTree = DecisionTree(min_samples_leaf = self.min_samples_leaf, max_depth = self.max_depth, level = self.level + 1, parent_value = self.parent_value)\n            self.leftTree.var_split(x_lhs, y[:self.split_val])\n            self.rightTree = DecisionTree(min_samples_leaf = self.min_samples_leaf, max_depth = self.max_depth, level = self.level + 1, parent_value = self.parent_value)\n            self.rightTree.var_split(x_rhs, y[self.split_val:])\n        \n        else :\n            self.score = float('inf')\n            #print('at leaf node : '+str(y))\n            y = [val for val in y if val != None]\n            self.value = np.mean(y)\n\n        \n    def predict_row(self, row):      \n        if self.is_leaf: \n            #print('result: ' + str(self.value))\n            return self.value\n        if row[self.split_feature] < self.split_val:\n            return self.leftTree.predict_row(row)\n        else:\n            return self.rightTree.predict_row(row)\n    \n    def predict_row_for_ti(self, row, feat_contribs):\n        \n        if self.is_leaf: \n           return self.value, self.parent_value , feat_contribs\n        if row[self.split_feature] < self.split_val:\n            if self.split_feature in feat_contribs.keys():\n                feat_contribs[self.split_feature] += self.leftTree.value - self.value\n            else:\n                feat_contribs[self.split_feature] = self.leftTree.value - self.value\n            return self.leftTree.predict_row_for_ti(row, feat_contribs)\n        else:\n            if self.split_feature in feat_contribs.keys():\n                feat_contribs[self.split_feature] += self.rightTree.value - self.value\n            else:\n                feat_contribs[self.split_feature] = self.rightTree.value - self.value\n            return self.rightTree.predict_row_for_ti(row, feat_contribs)\n           \n    def predict(self, X):\n        y_pred = []\n        for row in range(X.shape[0]):\n            y_pred.append(self.predict_row(X.iloc[row, :]))\n        return y_pred\n    \n    def get_prediction_and_bias(self):\n        \n        return self.parent_value, self.value \n        \n    def get_child_trees(self):\n        return self.leftTree, self.rightTree\n  \n    def __repr__(self):\n        return \"score: \" +str(self.score) + \" avg: \"+str(self.value) +  \" split val: \" + str(self.split_val) + \" split feature : \"+ str(self.split_feature)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29205bc2922106d3074ad4c4b944bf8e60b2e3fc"
      },
      "cell_type": "code",
      "source": "class TreeInterpreter:\n    \n    def predict(self, rf_model_tree, row):\n            prediction , bias, contribs = rf_model_tree.predict_row_for_ti(row, {})\n            print('prediction: '+str(prediction)+\"bias: \"+str(bias)+\" contributions: \"+str(contribs))\n           \n            return prediction, bias, contribs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0cd415d95dd24ab11565741f3c9ee65ba7e44f12"
      },
      "cell_type": "code",
      "source": "#class DecisionTree():\n #   def __init__()\ntrain_data = pd.read_csv('../input/train.csv')\n\ntrain_data, valid_data  = train_test_split(train_data, train_size = 0.7, random_state = 1)\n\ntrain_cats(train_data)\napply_cats(valid_data, train_data)\n\nX_train, y_train, _ = proc_df(train_data, 'SalePrice', max_n_cat = 7)\nX_valid, y_valid, _ = proc_df(valid_data, 'SalePrice', max_n_cat = 7)\n\ny_train = np.log(y_train)\ny_valid = np.log(y_valid)\n\nrf = RandomForestRegressor(n_jobs = -1)\nrf.fit(X_train, y_train)\nfeat_imp = rf.feature_importances_\nimp = sorted(zip(feat_imp, X_train.columns), reverse = True)\nimp[:10]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0504d284242ec28c87b312f521f94630c4827c62"
      },
      "cell_type": "code",
      "source": "#X_train = X_train[['OverallQual', 'GrLivArea']]\nrf_simple = RandomForestRegressor(n_estimators = 5, n_jobs = -1, random_state = 1, max_depth = 4)\nrf_simple.fit(X_train, y_train)\nprint(rmse(y_train, rf_simple.predict(X_train)))\nprint(rmse(y_valid, rf_simple.predict(X_valid)))\ndraw_tree(rf_simple[0], df = X_train, ratio = 0.8)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e35cf990d703048638191f02c2840ce55001d70"
      },
      "cell_type": "code",
      "source": "%prun\nrf_mine = RandomForest(500, n_trees = 5, min_samples_leaf = 3, max_depth = 6, max_features = 0.5)\nrf_mine.fit(X_train, y_train)\nprint(rf_mine.trees[4])\nprint(rf_mine.trees[0].leftTree.leftTree.leftTree.leftTree)\nprint(rmse(y_train, rf_mine.predict(X_train)))\nprint(rmse(y_valid, rf_mine.predict(X_valid)))\nplt.scatter(y_valid, rf_mine.predict(X_valid))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "deaf880d8e9eced973ed6a6baf924051b8d50a5a"
      },
      "cell_type": "markdown",
      "source": "Finding the confidence of a prediction by finding standard deviation of its group."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d2c2ec9771df51b1ff1787c33b57c3af68473c1"
      },
      "cell_type": "code",
      "source": "\ny_pred = np.stack([t.predict(X_valid) for t in rf_mine.trees])\nmean_pred, std_dev = np.mean(y_pred[:,0]), np.std(y_pred[:,0])\nX_new = X_valid.copy()\nX_new['mean_pred'] = mean_pred\nX_new['std_dev_pred'] = std_dev\nX_new.OverallQual.value_counts().plot.barh()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "749431a9e72d828486d010fd0a3298080f8b62b4"
      },
      "cell_type": "code",
      "source": "X_new['SalePrice'] = y_valid\nflds = ['OverallQual','SalePrice', 'mean_pred', 'std_dev_pred']\noverallQual_props = X_new[flds].groupby(['OverallQual'], as_index = False).mean()\noverallQual_props\noverallQual_props.plot('OverallQual', 'std_dev_pred', 'barh')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e256d332631423dbbfc25c54676168408754c14a"
      },
      "cell_type": "markdown",
      "source": "Find feature importances"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8cbc0753fdd2cc77393b6ce81a238c0ee8186319"
      },
      "cell_type": "code",
      "source": "%%cython\nimport numpy as np\n\ndef find_feature_importances(self, X, y):\n    feat_imp = {}\n    y_pred_old = self.predict(X)\n    for ind in range(X.shape[1]):\n        X_new = X.copy()\n        np.random.shuffle(X_new.values[:, ind])\n        y_pred = self.predict(X_new)\n        feat_imp[X.columns[ind]] = np.fabs(np.sum(y_pred_old) - np.sum(y_pred))\n        del X_new\n    \n    features, importances =zip(*sorted(feat_imp.items(), reverse=True))\n    return features, importances\n    \nRandomForest.find_feature_importances = find_feature_importances\nidxs = np.random.permutation(X_train.shape[0])[:50]\nfeatures, imp = find_feature_importances(rf_mine, X_train.iloc[idxs, :], y_train[idxs])\n\nplt.barh(features[:10], imp[:10])\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "de846aaa69745d8d699258d34f7ef37935ed606b"
      },
      "cell_type": "markdown",
      "source": "All predictions right now are the same value due to the data distribution, hence feature importances not shown."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff512c23385f9a0d10e8451950d552ff6ed94ebd"
      },
      "cell_type": "markdown",
      "source": "Finding partial feature dependencies and also feature interactions, by keeping other levers constant"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ea83cc08f7a72c474404f9cab7c1d01f00cb7b0"
      },
      "cell_type": "code",
      "source": "%%cython\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef plot_pdp(self, X, y, feature_name, n_clusters = 2):\n    feature_id = -1\n    y_pred_vals = []\n    y_pred_mean = []\n    for index in range(X.shape[1]):\n        if X.columns[index] == feature_name:\n            feature_id = index\n    X_new = X.copy()\n    for feature_val in np.unique(X.values[:, feature_id]):\n        \n        X_new.values[:, feature_id] = feature_val\n        y_pred_val = self.predict(X_new)\n        \n        y_pred_vals.append(y_pred_val)\n           \n   # print(\"y_pred_vals : \"+str(y_pred_vals))   \n    del X_new    \n   # plt.scatter(y, y_pred_mean)\n    y_pred_randoms = np.random.normal(y_pred_vals)[:n_clusters]\n  #  for y_pred in y_pred_randoms:\n   #     plt.plot(X.values[:, feature_id], y_pred)\n        \n        \n    y_pred_mean = np.mean(y_pred_vals, axis = 0)\n    print(\"y_pred_mean: \"+str(y_pred_mean))\n    plt.plot(X.values[:, feature_id], y_pred_mean)\n    plt.show()\n        \nRandomForest.plot_pdp = plot_pdp\nidxs = np.random.permutation(X_train.shape[0])[:10]\nplot_pdp(rf_mine, X_train.iloc[idxs, :], y_train[idxs], 'OverallQual')\n        ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e69ba4393a5cb1b5df11c3d123e953886fc22a8"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5827d67a144370b037e3fa3f4ada57baa6856a20"
      },
      "cell_type": "code",
      "source": "%%cython\nimport numpy as np\ndef plot_pdp_interaction(self, X, y, feature_names, n_clusters = 2):\n    features_list = list(enumerate(X.columns))\n    feature1 = -1; feature2 = -1\n    for ind, feature_name in features_list:\n        if feature_name == feature_names[0]:\n            feature1 = ind\n        if feature_name == feature_names[1]:\n            feature2 = ind\n        if feature1 > -1 and feature2 > -1:\n            break\n    y_pred_vals = []\n    X_new = X.copy()\n    feature1_list = []; feature2_list = []\n    for feature1_val in np.unique(X_new.values[:, feature1]):\n        feature1_list.append(feature1_val)\n        for feature2_val in np.unique(X_new.values[:, feature2]):\n            X_new = X.copy()\n            X_new.values[:, feature1], X_new.values[:, feature2] = feature1_val, feature2_val\n            y_pred = self.predict(X_new)\n            y_pred_vals.append(y_pred)\n            feature2_list.append(feature2_val)\n    del X_new\n    y_pred_mean = np.mean(y_pred_vals, axis = 0)\n    #plt.plot(feature1_list, feature2_list, fill_between = y_pred_mean, legend = 'True')\n    return y_pred_mean\n    \nidxs = np.random.permutation(X_train.shape[0])[:10]\nX_sample, y_sample = X_train.iloc[idxs, :], y_train[idxs]\nRandomForest.plot_pdp_interaction = plot_pdp_interaction\np = plot_pdp_interaction(rf_mine, X_sample, y_sample, ['OverallQual', 'LotArea'])\npdp.pdp_interact_plot(p,  ['OverallQual', 'LotArea'])    \n#Need to get deeper into PDP library and plots    \n\n        \n        ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0c6dd7de24cc8130ec7aef63d3777397964d67c6"
      },
      "cell_type": "markdown",
      "source": "Tree Interpreter and way 2 of Feature Importances. Waterfall Chart"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ed5196700a43aea3fc30b6dc4b56c226a846c76"
      },
      "cell_type": "code",
      "source": "ti = TreeInterpreter()\npr, bias, contribs  = ti.predict(rf_mine.trees[0], X_valid.iloc[0, :])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "02fcf32291a06c0feb0143ac46d4a50d4123c273"
      },
      "cell_type": "markdown",
      "source": "TODO: Waterfall Chart and Feature Interactions, by avergaing trends of feature contributions. Also, feature importance by averaging contributions across all trees."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d6c5eb6436b4646007fdf4f4aa2776a06de8a15"
      },
      "cell_type": "code",
      "source": "#!git clone https://www.github.com/chrispaulca/waterfall.git\nfrom waterfall import waterfall_chart\nwaterfall_chart.plot(list(contribs.keys()), list(contribs.values()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d7a1ce0fa561bc7656ee228eabbb3bdbfd64f17a"
      },
      "cell_type": "markdown",
      "source": "**We shall now find feature importances by averaging the contributions of features across all the trees in the RF Model**\nThis ought to lead us towards feature interactions."
    },
    {
      "metadata": {
        "_uuid": "26baed362994836453ca1046384fc7d0d770bfaa"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e951a3882dc5f0da9e2c4a96e4b93e3a41c588d0"
      },
      "cell_type": "code",
      "source": "avg_feat_imp = {}\nfor tree in rf_mine.trees:\n    for row in range(X_valid.shape[0]):\n        _,_, contribs  = ti.predict(tree, X_valid.iloc[row, :])\n        for key, value in contribs.items():\n            if key not in avg_feat_imp.keys():\n                avg_feat_imp[key] = [value]\n            else:\n                avg_feat_imp[key].append(value)\n                \nfor key in avg_feat_imp.keys():\n    avg_feat_imp[key] = np.sum(avg_feat_imp[key])/len(avg_feat_imp[key])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "18d3470cce9d094f9c10216be6c73996f2155fd1"
      },
      "cell_type": "markdown",
      "source": "**Now for the most challenging part, the feature interactions using the contributions of features in each row.**"
    },
    {
      "metadata": {
        "_uuid": "690f7bd06dd77ef98626f778b7f76b617eccddf5"
      },
      "cell_type": "markdown",
      "source": "Idea: form tuples of important features, iterate through all trees to find lists of their split features and see if these features occur near the top in a specific sequence. Maintain a count in dataframe. Features most commonly occuring together should have high interaction."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}